{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fff933b-143a-4ec1-97a0-d7002d4088cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lecture 8: QR factorizations\"\n",
    "author: \"Jamie Haddock\"\n",
    "format: \n",
    "    revealjs:\n",
    "        output-file: Lecture8_slides\n",
    "        slide-number: true\n",
    "        chalkboard: \n",
    "            buttons: false\n",
    "        preview-links: auto\n",
    "        logo: figs/hmc.png\n",
    "        css: input/slides.css\n",
    "        incremental: true\n",
    "        smaller: true\n",
    "        code-fold: true\n",
    "    html: \n",
    "        code-fold: true\n",
    "    pdf:\n",
    "        documentclass: article\n",
    "        toc: true\n",
    "        number-sections: true\n",
    "        geometry:\n",
    "          - top=1in\n",
    "          - left=1in\n",
    "          - bottom=1in\n",
    "          - right=1in\n",
    "format-links: false\n",
    "jupyter: julia-1.9\n",
    "filters: \n",
    "  - input/remove-pause.lua\n",
    "execute:\n",
    "  echo: true\n",
    "  eval: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a969",
   "metadata": {},
   "source": [
    "## Difference of orthogonal vectors\n",
    "\n",
    "::: {.callout-caution icon=false}\n",
    "## Exercise: Difference of orthogonal vectors\n",
    "Suppose that $\\mathbf{q}_1$ and $\\mathbf{q}_2$ are orthogonal vectors.  Prove then that $$\\|\\mathbf{q}_1 - \\mathbf{q}_2\\|^2 = \\|\\mathbf{q}_1\\|^2 + \\|\\mathbf{q}_2\\|^2.$$\n",
    ":::\n",
    "<details><summary>Answer:</summary> \n",
    "\n",
    "We can expand the norm using the inner product as $$\\|\\mathbf{q}_1 - \\mathbf{q}_2\\|^2 = (\\mathbf{q}_1 - \\mathbf{q}_2)^\\top (\\mathbf{q}_1 - \\mathbf{q}_2) = \\mathbf{q}_1^\\top \\mathbf{q}_1 - 2 \\mathbf{q}_1^\\top \\mathbf{q}_2 + \\mathbf{q}_2^\\top \\mathbf{q}_2 = \\|\\mathbf{q}_1\\|^2 + \\|\\mathbf{q}_2\\|^2.$$\n",
    "</details>\n",
    "\n",
    ". . .\n",
    "\n",
    "There is no possibility of subtractive cancellation here.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Fact: \n",
    "Addition and subtraction of vectors are guaranteed to be well-conditioned when the vectors are orthogonal.\n",
    ":::\n",
    "\n",
    "## ONC matrices\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: ONC matrix\n",
    "An **ONC matrix** is one whose columns are an orthonormal set of vectors.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "The following result follows from identifying the possible inner products between columns an ONC matrix.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: ONC matrix\n",
    "Suppose $\\mathbf{Q}$ is a real $n \\times k$ ONC matrix.  Then\n",
    "\n",
    "1. $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$ (the $k\\times k$ identity matrix)\n",
    "2. $\\|\\mathbf{Q}\\mathbf{x}\\|_2 = \\|\\mathbf{x}\\|_2$ for all $k$ vectors $\\mathbf{x}$\n",
    "3. $\\|\\mathbf{Q}\\|_2 = 1$\n",
    ":::\n",
    "\n",
    "<details><summary>Proof:</summary> \n",
    "\n",
    "1. The first result follows since $(\\mathbf{Q}^\\top \\mathbf{Q})_{ij} = \\mathbf{q}_i^\\top \\mathbf{q}_j$.\n",
    "2. $\\|\\mathbf{Q}\\mathbf{x}\\|_2^2 = (\\mathbf{Q}\\mathbf{x})^\\top \\mathbf{Q}\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{Q}^\\top \\mathbf{Q} \\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} = \\|\\mathbf{x}\\|_2^2$\n",
    "3. $\\|\\mathbf{Q}\\|_2 = \\max_{\\|\\mathbf{x}\\| = 1} \\|\\mathbf{Q}\\mathbf{x}\\|_2 = \\max_{\\|\\mathbf{x}\\| = 1} \\|\\mathbf{x}\\|_2 = 1$\n",
    "</details>\n",
    "\n",
    "## Orthogonal matrix\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Orthogonal matrix\n",
    "An **orthogonal matrix** is a square matrix with orthonormal columns.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "Orthogonal matrices have even strong properties than ONC matrices.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Orthogonal matrix\n",
    "Suppose $\\mathbf{Q}$ is an $n \\times n$ real orthogonal matrix.  Then\n",
    "\n",
    "1. $\\mathbf{Q}^\\top = \\mathbf{Q}^{-1}$\n",
    "2. $\\mathbf{Q}^\\top$ is also an orthogonal matrix\n",
    "3. $\\kappa(\\mathbf{Q}) = 1$ in the 2-norm\n",
    "4. For any othre $n \\times n$ matrix $\\mathbf{A}$, $\\|\\mathbf{A}\\mathbf{Q}\\|_2 = \\|\\mathbf{A}\\|_2$\n",
    "5. If $\\mathbf{U}$ is another $n \\times n$ orthogonal matrix, then $\\mathbf{Q}\\mathbf{U}$ is also orthogonal.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6588f",
   "metadata": {},
   "source": [
    "# Orthogonal factorization\n",
    "\n",
    "We now come to another important variant of matrix factorization (like the LU factorization).\n",
    "\n",
    "## QR factorization\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: QR factorization\n",
    "Every real $m \\times n$ matrix $\\mathbf{A}$ ($m \\ge n$) can be written as $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$ where $\\mathbf{Q}$ is an $m \\times m$ orthogonal matrix and $\\mathbf{R}$ is an $m \\times n$ upper triangular matrix.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "In linear algebra, you may have learned to compute the QR factorization through the Gram-Schmidt process, but it turns out this approach is numerically unstable so we'll learn a different technique!\n",
    "\n",
    "---\n",
    "\n",
    "Suppose $m \\gg n$ and visualize the QR factorization,\n",
    "$$\\mathbf{A} = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_m \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1m} \\\\ 0 & r_{22} & \\cdots & r_2m \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & r_{nn} \\\\ 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots &  & \\vdots \\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}.$$\n",
    "\n",
    ". . .\n",
    "\n",
    "Note that the many rows of all zeros at the bottom of $\\mathbf{R}$ mean that $\\mathbf{q}_{n+1}, \\mathbf{q}_{n+2}, \\cdots, \\mathbf{q}_m$ do not contribute to the factorization.\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Thin QR factorization\n",
    "The thin QR factorization is $\\mathbf{A} = \\hat{\\mathbf{Q}}\\hat{\\mathbf{R}}$ where $\\hat{\\mathbf{Q}}$ is an $m \\times n$ ONC matrix and $\\hat{\\mathbf{R}}$ is an $n \\times n$ upper triangular matrix.\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53119390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(m, n) = size(A) = (6, 4)\n"
     ]
    }
   ],
   "source": [
    "A = rand(1.:9.,6,4)\n",
    "@show m,n = size(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8897a983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "Q,R = qr(A);\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c1f5b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " -9.21954  -8.89415  -9.32801  -14.3174\n",
       "  0.0      -9.63816  -8.71902   -4.11477\n",
       "  0.0       0.0       8.42419    6.95263\n",
       "  0.0       0.0       0.0       -7.46601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4334ccd",
   "metadata": {},
   "source": [
    ". . .\n",
    "\n",
    "Strangely, $\\mathbf{Q}$ is $6 \\times 6$ (full QR) and $\\mathbf{R}$ is $4 \\times 4$ (thin QR).  However, $\\mathbf{Q}$ is given in a nonstandard form and converting to a standard matrix will recover the thin QR $\\hat{\\mathbf{Q}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb53a034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " -0.867722   0.178213    -0.0641336   0.434546\n",
       " -0.325396  -0.0109857    0.103146   -0.34541\n",
       " -0.108465  -0.833696    -0.270741    0.147474\n",
       " -0.325396  -0.0109857   -0.252971   -0.810981\n",
       " -0.108465  -0.00366191   0.825754   -0.0925295\n",
       " -0.108465  -0.522433     0.407533   -0.0621405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q̂ = Matrix(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28063fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\\mathbf{Q}$ is an orthogonal matrix and $\\hat{\\mathbf{Q}}$ is an ONC matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d5045ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.784676762789553e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opnorm(Q'*Q - I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb9a5e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.980709636773836e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opnorm(Q̂'*Q̂ - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd52876",
   "metadata": {},
   "source": [
    "## Least squares and QR\n",
    "\n",
    "Suppose we have a thin QR factorization of $\\mathbf{A} = \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}}$ and we are solving least-squares via the normal equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{A}^\\top \\mathbf{A} \\mathbf{x} &= \\mathbf{A}^\\top \\mathbf{b} \\\\\n",
    "\\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}} \\mathbf{x} &= \\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\mathbf{b} \\\\\n",
    "\\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{R}} \\mathbf{x} &= \\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\mathbf{b}\n",
    "\\end{align*}\n",
    "\n",
    ". . .\n",
    "\n",
    "Now, if $\\mathbf{A}$ is full-rank, then $\\hat{\\mathbf{R}}$ is nonsingular and we have $$\\hat{\\mathbf{R}} \\mathbf{x} = \\hat{\\mathbf{Q}}^\\top \\mathbf{b}.$$\n",
    "\n",
    ". . .\n",
    "\n",
    "Thus, the algorithm for solving least-squares by thin QR is:\n",
    "\n",
    "1. Compute the thin QR factorization $\\mathbf{A} = \\hat{\\mathbf{Q}}\\hat{\\mathbf{R}}$.\n",
    "2. Compute $\\mathbf{z} = \\hat{\\mathbf{Q}}^\\top \\mathbf{b}$.\n",
    "3. Solve the $n \\times n$ linear system $\\hat{\\mathbf{R}} \\mathbf{x} = \\mathbf{z}$ for $\\mathbf{x}$ via backsubstitution.  \n",
    "\n",
    "---\n",
    "\n",
    "This algorithm is essentially what is implemented in the Julia `\\` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ca9472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "using FundamentalsNumericalComputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32349c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsqrfact"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    lsqrfact(A,b)\n",
    "Solve a linear least-squares problem by QR factorization.  Returns the minimizer of ||b - Ax||.\n",
    "\"\"\"\n",
    "function lsqrfact(A,b)\n",
    "    Q,R = qr(A)\n",
    "    z = Q'*b\n",
    "    x = FNC.backsub(R,z)\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819360d1",
   "metadata": {},
   "source": [
    "Recall previously we saw that the normal equations can be unstable -- this method doesn't change that fact, but allows us to solve them up to the loss of accuracy predicted due to the instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4d7ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = range(0,3,length=400)\n",
    "f = [ x->sin(x)^2, x->cos((1+1e-7)*x)^2, x->1. ]\n",
    "A = [ f(t) for t in t, f in f ]\n",
    "x = [1., 2, 1]\n",
    "b = A*x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a768706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed_error = 6.4042143577854036e-9\n",
      "error_bound = κ * eps() = 4.053030227715619e-9\n"
     ]
    }
   ],
   "source": [
    "observed_error = norm(lsqrfact(A,b) - x)/norm(x);\n",
    "@show observed_error;\n",
    "κ = cond(A)\n",
    "@show error_bound = κ*eps();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da788876",
   "metadata": {},
   "source": [
    "# Computing QR factorizations\n",
    "\n",
    "One can compute a thin QR factorization using the outer product formula (like we did with LU) factorization, which is essentially the Gram-Schmidt process.  However, this algorithm is unstable, and a better approach is to use products of orthogonal matrices to introduce zeros into the lower triangular portion of the matrix.  (We exploit the fact that products of orthogonal matrices are orthogonal.)\n",
    "\n",
    "## Householder reflections\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Householder reflector\n",
    "A **Householder reflector** is a matrix of the form $$\\mathbf{P} = \\mathbf{I} - 2\\mathbf{v}\\mathbf{v}^\\top,$$ where $\\mathbf{v}$ is any unit vector (in the 2-norm).\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Householder reflector part 1\n",
    "A Householder reflector is:\n",
    "\n",
    "1. symmetric\n",
    "2. orthogonal\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "Note that $\\mathbf{P}\\mathbf{x} = \\mathbf{x} - 2 \\mathbf{v}(\\mathbf{v}^\\top\\mathbf{x})$.  Visualizing this equation explains why these are called *reflectors*.\n",
    "\n",
    "![](figs/householder.png){height=400}\n",
    "\n",
    "---\n",
    "\n",
    "Now, we may choose $\\mathbf{v}$ so that this reflection $\\mathbf{P}\\mathbf{z}$ is very sparse.  In fact, we choose $\\mathbf{v}$ so that $$\\mathbf{P}\\mathbf{z} = \\begin{bmatrix} \\pm \\|\\mathbf{z}\\| \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\pm \\|\\mathbf{z}\\| \\mathbf{e}_1.$$\n",
    "\n",
    "---\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Householder reflection part 2\n",
    "If $\\mathbf{w} = \\|\\mathbf{z}\\| \\mathbf{e}_1 - \\mathbf{z}$ and $\\mathbf{v} = \\mathbf{w}/\\|\\mathbf{w}\\|$ then $$\\mathbf{P}\\mathbf{z} = \\|\\mathbf{z}\\| \\mathbf{e}_1.$$\n",
    ":::\n",
    "\n",
    "<details><summary>Proof:</summary> \n",
    "\n",
    "Note that \n",
    "\\begin{align*}\n",
    "\\mathbf{w}^\\top \\mathbf{w} &= 2\\|\\mathbf{z}\\|^2 - 2\\|\\mathbf{z}\\|z_1\\\\\n",
    "\\mathbf{w}^\\top \\mathbf{z} &= \\|\\mathbf{z}\\| z_1 - \\|\\mathbf{z}\\|^2,\n",
    "\\end{align*}\n",
    "and thus, $$\\mathbf{P}\\mathbf{z} = \\mathbf{z} - 2 (\\mathbf{v}^\\top \\mathbf{z}) \\mathbf{v} = \\mathbf{z} - 2 \\frac{\\mathbf{w}^\\top \\mathbf{z}}{\\mathbf{w}^\\top \\mathbf{w}} \\mathbf{w} = \\mathbf{z} - 2 \\frac{\\|\\mathbf{z}\\| z_1 - \\|\\mathbf{z}\\|^2}{2\\|\\mathbf{z}\\|^2 - 2\\|\\mathbf{z}\\|z_1} \\mathbf{w} = \\mathbf{z} + \\mathbf{w} = \\|\\mathbf{z}\\|\\mathbf{e}_1.$$\n",
    "</details>\n",
    "\n",
    "## Factorization algorithm demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dedd9d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = rand(float(1:9),6,4)\n",
    "m,n = size(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12e2d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 Matrix{Float64}:\n",
       " 0.263117   0.613941   0.175412    0.526235   0.350823    0.350823\n",
       " 0.613941   0.48849   -0.146146   -0.438437  -0.292292   -0.292292\n",
       " 0.175412  -0.146146   0.958244   -0.125268  -0.0835119  -0.0835119\n",
       " 0.526235  -0.438437  -0.125268    0.624196  -0.250536   -0.250536\n",
       " 0.350823  -0.292292  -0.0835119  -0.250536   0.832976   -0.167024\n",
       " 0.350823  -0.292292  -0.0835119  -0.250536  -0.167024    0.832976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = A[:,1];\n",
    "v = normalize(z - norm(z)*[1;zeros(m-1)])\n",
    "P₁ = I - 2v*v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66663c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Float64}:\n",
       " 11.401754250991381\n",
       "  0.0\n",
       "  0.0\n",
       "  4.440892098500626e-16\n",
       " -4.440892098500626e-16\n",
       " -4.440892098500626e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "P₁*z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005f0b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we recurse!  We redefine $\\mathbf{A}$ and work on the submatrix that does not include its first row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec40df16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " 11.4018       12.1911     9.99846  11.8403\n",
       "  0.0          -1.8245    -1.4982   -1.86589\n",
       " -1.11022e-16   6.05014    2.28623  -0.390253\n",
       "  0.0           0.150431   5.85869   4.82924\n",
       "  0.0           1.10029    2.57246   0.219493\n",
       "  0.0           1.10029    2.57246   4.21949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = P₁*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43c26a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " -0.280271    0.929396    0.0231086    0.169021     0.169021\n",
       "  0.929396    0.325317   -0.0167754   -0.122699    -0.122699\n",
       "  0.0231086  -0.0167754   0.999583    -0.00305079  -0.00305079\n",
       "  0.169021   -0.122699   -0.00305079   0.977686    -0.0223142\n",
       "  0.169021   -0.122699   -0.00305079  -0.0223142    0.977686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = A[2:m,2]\n",
    "v = normalize(z - norm(z)*[1;zeros(m-2)])\n",
    "P₂ = I - 2v*v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a5f20a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " 11.4018       12.1911        9.99846  11.8403\n",
       " -1.03184e-16   6.50976       3.5497    1.02213\n",
       " -3.61174e-17   1.5525e-15   -1.37823  -2.48678\n",
       "  1.86244e-18   3.75082e-17   5.76758   4.77711\n",
       "  1.36223e-17   1.25457e-16   1.90604  -0.161783\n",
       "  1.36223e-17   2.7015e-16    1.90604   3.83822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A[2:m,:] = P₂*A[2:m,:]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a1a3e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3900530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in 3:n\n",
    "    z = A[j:m,j]\n",
    "    v = normalize(z - norm(z)*[1;zeros(m-j)])\n",
    "    P = I - 2v*v'\n",
    "    A[j:m,:] = P*A[j:m,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f833da28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " 11.4018  12.1911   9.99846  11.8403\n",
       "  0.0      6.50976  3.5497    1.02213\n",
       "  0.0      0.0      6.51386   5.83174\n",
       "  0.0      0.0      0.0       3.12311\n",
       "  0.0      0.0      0.0       0.0\n",
       "  0.0      0.0      0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = triu(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb4f09",
   "metadata": {},
   "source": [
    "Now, note that we didn't explicitly build $\\hat{\\mathbf{Q}}$ here.  $\\hat{\\mathbf{Q}}$ is derived from the products of the Householder reflectors, but these matrices are getting smaller as they only need to act on a submatrix of $\\mathbf{A}$.  To explicitly build $\\hat{\\mathbf{Q}}$, we have to pad out the Householder reflectors $\\mathbf{P}_k$ to make them appropriately sized.\n",
    "\n",
    "---\n",
    "\n",
    "Define $\\mathbf{Q}_k = \\begin{bmatrix} \\mathbf{I}_{k-1} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{P}_k \\end{bmatrix}$.  First, note that $\\mathbf{Q}_k$ is orthogonal and that $$\\mathbf{Q}_n \\mathbf{Q}_{n-1} \\cdots \\mathbf{Q}_1 \\mathbf{A} = \\mathbf{R}.$$  Thus, we have that $\\mathbf{Q}^\\top = \\mathbf{Q}_n \\mathbf{Q}_{n-1} \\cdots \\mathbf{Q}_1$.  Now, rather than explicitly building these $\\mathbf{Q}_k$ matrices explicitly and constructing their product, we can simply begin from $\\mathbf{I}$ and do the actions of the $\\mathbf{Q}_k$ matrices (the $\\mathbf{P}_k$ matrices) in the same way we did on $\\mathbf{A}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc859ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qrfact"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    qrfact(A)\n",
    "QR factorization by Householder reflections.  Returns Q and R.\n",
    "\"\"\"\n",
    "function qrfact(A)\n",
    "    m,n = size(A)\n",
    "    Qt = diagm(ones(m))\n",
    "    R = float(copy(A))\n",
    "    for k in 1:n\n",
    "        z = R[k:m,k]\n",
    "        w = [ -sign(z[1])*norm(z) - z[1]; -z[2:end] ]\n",
    "        nrmw = norm(w)\n",
    "        if nrmw < eps() continue; end  #skip this iteration\n",
    "        v = w / nrmw;\n",
    "        # Apply the reflection to each relevant column of A and Q\n",
    "        for j in 1:n\n",
    "            R[k:m,j] -= v*( 2*(v'*R[k:m,j]) )\n",
    "        end\n",
    "        for j in 1:m\n",
    "            Qt[k:m,j] -= v*( 2*(v'*Qt[k:m,j]) )\n",
    "        end\n",
    "    end\n",
    "    return Qt', triu(R)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f457efe",
   "metadata": {},
   "source": [
    "## Q-less QR\n",
    "\n",
    "Now, note that in many applications we don't explicitly need the matrix $\\mathbf{Q}$.  \n",
    "\n",
    ". . .\n",
    "\n",
    "For instance, in least-squares, we only need $\\mathbf{Q}^\\top \\mathbf{b}$.  We don't actually need $\\mathbf{Q}^\\top$, we just need the image of $\\mathbf{b}$ under $\\mathbf{Q}^\\top$ as an operator!  This is an important topic in scientific computing -- understanding when a matrix needs to be formed and stored and when it does not!\n",
    "\n",
    ". . .\n",
    "\n",
    "In using QR factorization to solve least-squares, we don't need to form $\\mathbf{Q}^\\top$, we can just use the action of the $\\mathbf{Q}_k$ matrices (the $\\mathbf{P}_k$ matrices) to construct $\\mathbf{R}$ from $\\mathbf{A}$ and $\\mathbf{z} = \\mathbf{Q}^\\top \\mathbf{b}$ from $\\mathbf{b}$.\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: \n",
    "Q-less QR factorization by Householder reflections takes $\\sim (2mn^2 - \\frac23 n^3)$ flops.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c12669-a869-46d9-aa62-bbb2a02b1783",
   "metadata": {},
   "source": [
    "<!--\n",
    "[verbose test]{.content-hidden when-format=\"revealjs\" when-format=\"pptx\"}\n",
    "\n",
    "::: {.callout-caution icon=false}\n",
    "## Exercise: \n",
    "\n",
    ":::\n",
    "\n",
    "<details><summary>Answer:</summary> </details>\n",
    "\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: \n",
    " \n",
    ":::\n",
    "\n",
    "\n",
    "::: {.callout-tip icon=false}\n",
    "## Note: \n",
    " \n",
    ":::\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
