{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fff933b-143a-4ec1-97a0-d7002d4088cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lecture 8: QR factorizations\"\n",
    "author: \"Jamie Haddock\"\n",
    "format: \n",
    "    revealjs:\n",
    "        output-file: Lecture8_slides\n",
    "        slide-number: true\n",
    "        chalkboard: \n",
    "            buttons: false\n",
    "        preview-links: auto\n",
    "        logo: figs/hmc.png\n",
    "        css: input/slides.css\n",
    "        incremental: true\n",
    "        smaller: true\n",
    "        code-fold: true\n",
    "    html: \n",
    "        code-fold: true\n",
    "    pdf:\n",
    "        documentclass: article\n",
    "        toc: true\n",
    "        number-sections: true\n",
    "        geometry:\n",
    "          - top=1in\n",
    "          - left=1in\n",
    "          - bottom=1in\n",
    "          - right=1in\n",
    "format-links: false\n",
    "jupyter: julia-1.9\n",
    "filters: \n",
    "  - input/remove-pause.lua\n",
    "execute:\n",
    "  echo: true\n",
    "  eval: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a969",
   "metadata": {},
   "source": [
    "## Difference of orthogonal vectors\n",
    "\n",
    "::: {.callout-caution icon=false}\n",
    "## Exercise: Difference of orthogonal vectors\n",
    "Suppose that $\\mathbf{q}_1$ and $\\mathbf{q}_2$ are orthogonal vectors.  Prove then that $$\\|\\mathbf{q}_1 - \\mathbf{q}_2\\|^2 = \\|\\mathbf{q}_1\\|^2 + \\|\\mathbf{q}_2\\|^2.$$\n",
    ":::\n",
    "<details><summary>Answer:</summary> \n",
    "\n",
    "We can expand the norm using the inner product as $$\\|\\mathbf{q}_1 - \\mathbf{q}_2\\|^2 = (\\mathbf{q}_1 - \\mathbf{q}_2)^\\top (\\mathbf{q}_1 - \\mathbf{q}_2) = \\mathbf{q}_1^\\top \\mathbf{q}_1 - 2 \\mathbf{q}_1^\\top \\mathbf{q}_2 + \\mathbf{q}_2^\\top \\mathbf{q}_2 = \\|\\mathbf{q}_1\\|^2 + \\|\\mathbf{q}_2\\|^2.$$\n",
    "</details>\n",
    "\n",
    ". . .\n",
    "\n",
    "There is no possibility of subtractive cancellation here.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Fact: \n",
    "Addition and subtraction of vectors are guaranteed to be well-conditioned when the vectors are orthogonal.\n",
    ":::\n",
    "\n",
    "## ONC matrices\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: ONC matrix\n",
    "An **ONC matrix** is one whose columns are an orthonormal set of vectors.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "The following result follows from identifying the possible inner products between columns an ONC matrix.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: ONC matrix\n",
    "Suppose $\\mathbf{Q}$ is a real $n \\times k$ ONC matrix.  Then\n",
    "\n",
    "1. $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$ (the $k\\times k$ identity matrix)\n",
    "2. $\\|\\mathbf{Q}\\mathbf{x}\\|_2 = \\|\\mathbf{x}\\|_2$ for all $k$ vectors $\\mathbf{x}$\n",
    "3. $\\|\\mathbf{Q}\\|_2 = 1$\n",
    ":::\n",
    "\n",
    "<details><summary>Proof:</summary> \n",
    "\n",
    "1. The first result follows since $(\\mathbf{Q}^\\top \\mathbf{Q})_{ij} = \\mathbf{q}_i^\\top \\mathbf{q}_j$.\n",
    "2. $\\|\\mathbf{Q}\\mathbf{x}\\|_2^2 = (\\mathbf{Q}\\mathbf{x})^\\top \\mathbf{Q}\\mathbf{x} = \\mathbf{x}^\\top \\mathbf{Q}^\\top \\mathbf{Q} \\mathbf{x} = \\mathbf{x}^\\top \\mathbf{x} = \\|\\mathbf{x}\\|_2^2$\n",
    "3. $\\|\\mathbf{Q}\\|_2 = \\max_{\\|\\mathbf{x}\\| = 1} \\|\\mathbf{Q}\\mathbf{x}\\|_2 = \\max_{\\|\\mathbf{x}\\| = 1} \\|\\mathbf{x}\\|_2 = 1$\n",
    "</details>\n",
    "\n",
    "## Orthogonal matrix\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Orthogonal matrix\n",
    "An **orthogonal matrix** is a square matrix with orthonormal columns.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "Orthogonal matrices have even strong properties than ONC matrices.\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Orthogonal matrix\n",
    "Suppose $\\mathbf{Q}$ is an $n \\times n$ real orthogonal matrix.  Then\n",
    "1. $\\mathbf{Q}^\\top = \\mathbf{Q}^{-1}$\n",
    "2. $\\mathbf{Q}^\\top$ is also an orthogonal matrix\n",
    "3. $\\kappa(\\mathbf{Q}) = 1$ in the 2-norm\n",
    "4. For any othre $n \\times n$ matrix $\\mathbf{A}$, $\\|\\mathbf{A}\\mathbf{Q}\\|_2 = \\|\\mathbf{A}\\|_2$\n",
    "5. If $\\mathbf{U}$ is another $n \\times n$ orthogonal matrix, then $\\mathbf{Q}\\mathbf{U}$ is also orthogonal.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6588f",
   "metadata": {},
   "source": [
    "# Orthogonal factorization\n",
    "\n",
    "We now come to another important variant of matrix factorization (like the LU factorization).\n",
    "\n",
    "## QR factorization\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: QR factorization\n",
    "Every real $m \\times n$ matrix $\\mathbf{A}$ ($m \\ge n$) can be written as $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$ where $\\mathbf{Q}$ is an $m \\times m$ orthogonal matrix and $\\mathbf{R}$ is an $m \\times n$ upper triangular matrix.\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "In linear algebra, you may have learned to compute the QR factorization through the Gram-Schmidt process, but it turns out this approach is numerically unstable so we'll learn a different technique!\n",
    "\n",
    "---\n",
    "\n",
    "Suppose $m \\gg n$ and visualize the QR factorization,\n",
    "$$\\mathbf{A} = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_m \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1m} \\\\ 0 & r_{22} & \\cdots & r_2m \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & r_{nn} \\\\ 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots &  & \\vdots \\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}.$$\n",
    "\n",
    ". . .\n",
    "\n",
    "Note that the many rows of all zeros at the bottom of $\\mathbf{R}$ mean that $\\mathbf{q}_{n+1}, \\mathbf{q}_{n+2}, \\cdots, \\mathbf{q}_m$ do not contribute to the factorization.\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Thin QR factorization\n",
    "The thin QR factorization is $\\mathbf{A} = \\hat{\\mathbf{Q}}\\hat{\\mathbf{R}}$ where $\\hat{\\mathbf{Q}}$ is an $m \\times n$ ONC matrix and $\\hat{\\mathbf{R}}$ is an $n \\times n$ upper triangular matrix.\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53119390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(m, n) = size(A) = (6, 4)\n"
     ]
    }
   ],
   "source": [
    "A = rand(1.:9.,6,4)\n",
    "@show m,n = size(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8897a983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×6 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "Q,R = qr(A);\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1f5b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " -16.4012  -10.7919   -10.548    -4.69477\n",
       "   0.0      -6.74799   -1.65491  -5.97728\n",
       "   0.0       0.0        5.00011   3.51747\n",
       "   0.0       0.0        0.0       0.926656"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4334ccd",
   "metadata": {},
   "source": [
    ". . .\n",
    "\n",
    "Strangely, $\\mathbf{Q}$ is $6 \\times 6$ (full QR) and $\\mathbf{R}$ is $4 \\times 4$ (thin QR).  Howevver, $\\mathbf{Q}$ is given in a nonstandard form and converting to a standard matrix will recover the thin QR $\\hat{\\mathbf{Q}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb53a034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " -0.54874    0.284816   0.136645   -0.382483\n",
       " -0.182913  -0.300241   0.914733    0.139313\n",
       " -0.182913  -0.89301   -0.281437   -0.222907\n",
       " -0.54874    0.136623  -0.112399   -0.393041\n",
       " -0.487769   0.039114  -0.216046    0.759469\n",
       " -0.304855  -0.105222  -0.0779476   0.230949"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q̂ = Matrix(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28063fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\\mathbf{Q}$ is an orthogonal matrix and $\\hat{\\mathbf{Q}}$ is an ONC matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d5045ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.451145070743261e-16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opnorm(Q'*Q - I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb9a5e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.932657863084677e-16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opnorm(Q̂'*Q̂ - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd52876",
   "metadata": {},
   "source": [
    "## Least squares and QR\n",
    "\n",
    "Suppose we have a thin QR factorization of $\\mathbf{A} = \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}}$ and we are solving least-squares via the normal equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{A}^\\top \\mathbf{A} \\mathbf{x} &= \\mathbf{A}^\\top \\mathbf{b} \\\\\n",
    "\\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}} \\mathbf{x} &= \\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\mathbf{b} \\\\\n",
    "\\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{R}} \\mathbf{x} &= \\hat{\\mathbf{R}}^\\top \\hat{\\mathbf{Q}}^\\top \\mathbf{b}\n",
    "\\end{align*}\n",
    "\n",
    ". . .\n",
    "\n",
    "Now, if $\\mathbf{A}$ is full-rank, then $\\hat{\\mathbf{R}}$ is nonsingular and we have $$\\hat{\\mathbf{R}} \\mathbf{x} = \\hat{\\mathbf{Q}}^\\top \\mathbf{b}.$$\n",
    "\n",
    ". . .\n",
    "\n",
    "Thus, the algorithm for solving least-squares by thin QR is:\n",
    "\n",
    "1. Compute the thin QR factorization $\\mathbf{A} = \\hat{\\mathbf{Q}}\\hat{\\mathbf{R}}$.\n",
    "2. Compute $\\mathbf{z} = \\hat{\\mathbf{Q}}^\\top \\mathbf{b}$.\n",
    "3. Solve the $n \\times n$ linear system $\\hat{\\mathbf{R}} \\mathbf{x} = \\mathbf{z}$ for $\\mathbf{x}$ via backsubstitution.  \n",
    "\n",
    "---\n",
    "\n",
    "This algorithm is essentially what is implemented in the Julia `\\` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ca9472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "using FundamentalsNumericalComputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32349c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lsqrfact"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    lsqrfact(A,b)\n",
    "Solve a linear least-squares problem by QR factorization.  Returns the minimizer of ||b - Ax||.\n",
    "\"\"\"\n",
    "function lsqrfact(A,b)\n",
    "    Q,R = qr(A)\n",
    "    z = Q'*b\n",
    "    x = FNC.backsub(R,z)\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819360d1",
   "metadata": {},
   "source": [
    "Recall previously we saw that the normal equations can be unstable -- this method doesn't change that fact, but allows us to solve them up to the loss of accuracy predicted due to the instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4d7ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = range(0,3,length=400)\n",
    "f = [ x->sin(x)^2, x->cos((1+1e-7)*x)^2, x->1. ]\n",
    "A = [ f(t) for t in t, f in f ]\n",
    "x = [1., 2, 1]\n",
    "b = A*x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a768706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed_error = 4.665273501889628e-9\n",
      "error_bound = κ * eps() = 4.053030228488391e-9\n"
     ]
    }
   ],
   "source": [
    "observed_error = norm(lsqrfact(A,b) - x)/norm(x);\n",
    "@show observed_error;\n",
    "κ = cond(A)\n",
    "@show error_bound = κ*eps();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da788876",
   "metadata": {},
   "source": [
    "# Computing QR factorizations\n",
    "\n",
    "One can compute a thin QR factorization using the outer product formula (like we did with LU) factorization, which is essentially the Gram-Schmidt process.  However, this algorithm is unstable, and a better approach is to use products of orthogonal matrices to introduce zeros into the lower triangular portion of the matrix.  (We exploit the fact that products of orthogonal matrices are orthogonal.)\n",
    "\n",
    "## Householder reflections\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: Householder reflector\n",
    "A **Householder reflector** is a matrix of the form $$\\mathbf{P} = \\mathbf{I} - 2\\mathbf{v}\\mathbf{v}^\\top,$$ where $\\mathbf{v}$ is any unit vector (in the 2-norm).\n",
    ":::\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Householder reflector part 1\n",
    "A Householder reflector is:\n",
    "\n",
    "1. symmetric\n",
    "2. orthogonal\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "Note that $\\mathbf{P}\\mathbf{x} = \\mathbf{x} - 2 \\mathbf{v}(\\mathbf{v}^\\top\\mathbf{x})$.  Visualizing this equation explains why these are called *reflectors*.\n",
    "\n",
    "![](figs/householder.png){height=400}\n",
    "\n",
    "---\n",
    "\n",
    "Now, we may choose $\\mathbf{v}$ so that this reflection $\\mathbf{P}\\mathbf{z}$ is very sparse.  In fact, we choose $\\mathbf{v}$ so that $$\\mathbf{P}\\mathbf{z} = \\begin{bmatrix} \\pm \\|\\mathbf{z}\\| \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\pm \\|\\mathbf{z}\\| \\mathbf{e}_1.$$\n",
    "\n",
    ". . .\n",
    "\n",
    "::: {.callout-warning icon=false}\n",
    "## Theorem: Householder reflection part 2\n",
    "If $\\mathbf{w} = \\|\\mathbf{z}\\| \\mathbf{e}_1 - \\mathbf{z}$ and $\\mathbf{v} = \\mathbf{w}/\\|\\mathbf{w}\\|$ then $$\\mathbf{P}\\mathbf{z} = \\|\\mathbf{z}\\| \\mathbf{e}_1.$$\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c12669-a869-46d9-aa62-bbb2a02b1783",
   "metadata": {},
   "source": [
    "<!--\n",
    "[verbose test]{.content-hidden when-format=\"revealjs\" when-format=\"pptx\"}\n",
    "\n",
    "::: {.callout-caution icon=false}\n",
    "## Exercise: \n",
    "\n",
    ":::\n",
    "\n",
    "<details><summary>Answer:</summary> </details>\n",
    "\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Definition: \n",
    " \n",
    ":::\n",
    "\n",
    "\n",
    "::: {.callout-tip icon=false}\n",
    "## Note: \n",
    " \n",
    ":::\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
